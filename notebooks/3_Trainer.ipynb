{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86f062b",
   "metadata": {},
   "source": [
    "## torch based DataLoader 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import (\n",
    "    PCOSDataset,\n",
    "    create_label_mapping,\n",
    "    stratified_split_by_pid,\n",
    "    stratified_pid_kfold,\n",
    ")\n",
    "from utils.dataset import HFVisionDataset\n",
    "from utils.dataset import create_weighted_sampler\n",
    "\n",
    "from utils.transform import get_transform, SpeckleNoise, AddGaussianNoise\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = \"/workspace/pcos_dataset/Dataset\"\n",
    "label_path = \"/workspace/pcos_dataset/labels/기존_Dataset_info.csv\"\n",
    "result_root_dir = \"/workspace/pcos_dataset/results\"\n",
    "\n",
    "label_df = pd.read_csv(label_path)\n",
    "\n",
    "# 1) transform 생성\n",
    "train_tf, val_tf = get_transform(\n",
    "    train_transform=[\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.05,0.05), scale=(0.95,1.05)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "        # AddGaussianNoise(std=0.02),\n",
    "        # SpeckleNoise(noise_factor=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) label mapping\n",
    "label_mapping = create_label_mapping(label_df, \"label\")\n",
    "\n",
    "# 3) Train / Val / Test split (PID 단위 7:1:2)\n",
    "train_df, val_df, test_df = stratified_split_by_pid(label_df)\n",
    "\n",
    "# 4) Tune = Train + Val (5-Fold용)\n",
    "tune_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "\n",
    "# 5) PID 기반 5-Fold\n",
    "folds = stratified_pid_kfold(tune_df, n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e633c4",
   "metadata": {},
   "source": [
    "## Huggingface based Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForImageClassification,\n",
    ")\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "\n",
    "num_labels = len(label_mapping)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(references=labels, predictions=preds)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(references=labels, predictions=preds, average=\"macro\")[\"f1\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "\n",
    "for fold_idx, (fold_train_df, fold_val_df) in enumerate(folds):\n",
    "    print(f\"\\n======== Fold {fold_idx} Training Start ========\")\n",
    "\n",
    "    train_base = PCOSDataset(\n",
    "        fold_train_df, data_root_dir,\n",
    "        filename_col=\"filename\",\n",
    "        label_col=\"label\",\n",
    "        label_mapping=label_mapping,\n",
    "        transform=train_tf,\n",
    "    )\n",
    "\n",
    "    val_base = PCOSDataset(\n",
    "        fold_val_df, data_root_dir,\n",
    "        filename_col=\"filename\",\n",
    "        label_col=\"label\",\n",
    "        label_mapping=label_mapping,\n",
    "        transform=val_tf,\n",
    "    )\n",
    "\n",
    "    train_dataset = HFVisionDataset(train_base)\n",
    "    val_dataset   = HFVisionDataset(val_base)\n",
    "\n",
    "    # fold마다 새 TrainingArguments / 새 output_dir\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{result_root_dir}/pcos_fold_{fold_idx}\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"{result_root_dir}/logs_fold_{fold_idx}\",\n",
    "    )\n",
    "\n",
    "    # fold마다 모델을 새로 초기화하는 게 더 정석\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        cache_dir=\"/workspace/pcos_dataset/models\"\n",
    "    ).to(device)\n",
    "\n",
    "    model.config.id2label = {int(v): str(k) for k, v in label_mapping.items()}\n",
    "    model.config.label2id = {str(k): int(v) for k, v in label_mapping.items()}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"[Fold {fold_idx}] metrics:\", metrics)\n",
    "    fold_results.append(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aecaa0",
   "metadata": {},
   "source": [
    "## Pytorch based Multiple Instance Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import PCOSMILDataset, create_label_mapping, create_weighted_sampler, stratified_pid_kfold, stratified_split_by_pid\n",
    "from utils.transform import get_transform, SpeckleNoise, AddGaussianNoise\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd \n",
    "data_root_dir = \"/workspace/pcos_dataset/Dataset\"\n",
    "label_path = \"/workspace/pcos_dataset/labels/기존_Dataset_info.csv\"\n",
    "label_df = pd.read_csv(label_path)\n",
    "\n",
    "train_tf, val_tf = get_transform(\n",
    "    train_transform=[\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        # transforms.RandomAffine(degrees=0, translate=(0.05,0.05), scale=(0.95,1.05)),\n",
    "        # transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "\n",
    "        # AddGaussianNoise(std=0.02),\n",
    "        # SpeckleNoise(noise_factor=0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "label_mapping = create_label_mapping(label_df, \"label\")\n",
    "\n",
    "# 2) Train / Val / Test split (PID 단위 stratified 7:1:2)\n",
    "train_df, val_df, test_df = stratified_split_by_pid(label_df)\n",
    "\n",
    "# 3) Tune = Train + Val (80%)\n",
    "tune_df = pd.concat([train_df, val_df]).reset_index(drop=True)\n",
    "\n",
    "# 4) K-Fold는 tune_df 에 대해서만 적용\n",
    "folds = stratified_pid_kfold(tune_df, n_splits=5) # [ADD] K-Fold Cross Validation 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_logit):\n",
    "    \"\"\"Fold나 Test에서 사용할 metric 계산\"\"\"\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "    try:\n",
    "        if len(np.unique(y_true)) == 2:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_logit[:, 1])\n",
    "        else:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_logit, multi_class=\"ovr\")\n",
    "    except:\n",
    "        metrics[\"roc_auc\"] = np.nan\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d82f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"images\"].to(device)   # (N_inst, C, H, W)\n",
    "        label = batch[\"label\"].to(device)   # (1,)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)                # (1, num_classes)\n",
    "        loss = criterion(logits, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    trues = []\n",
    "    logits_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs = batch[\"images\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, label)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds.append(torch.argmax(logits, dim=1).cpu().item())\n",
    "            trues.append(label.cpu().item())\n",
    "            logits_all.append(logits.cpu().numpy())\n",
    "\n",
    "    logits_all = np.vstack(logits_all)\n",
    "    metrics = compute_metrics(trues, preds, logits_all)\n",
    "\n",
    "    return np.mean(losses), metrics\n",
    "\n",
    "def train_fold(fold_idx, train_df, val_df, num_epochs=5):\n",
    "    print(f\"\\n========== Fold {fold_idx} ==========\")\n",
    "\n",
    "    # Dataset\n",
    "    train_dataset = PCOSMILDataset(train_df, data_root_dir, transform=train_tf)\n",
    "    val_dataset = PCOSMILDataset(val_df, data_root_dir, transform=val_tf)\n",
    "\n",
    "    # Loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "    # Model 선택\n",
    "    # model = AttentionMIL(num_classes=num_classes, embed_dim=256).to(device)\n",
    "    model = TransformerMIL(num_classes=num_classes, embed_dim=256, depth=4).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "    best_val_f1 = -1\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss, val_metrics = validate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"[Train] Loss: {train_loss:.4f}\")\n",
    "        print(f\"[Val] Loss: {val_loss:.4f}, Metrics: {val_metrics}\")\n",
    "\n",
    "        if val_metrics[\"f1_macro\"] > best_val_f1:\n",
    "            best_val_f1 = val_metrics[\"f1_macro\"]\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    # 최종 best model 반환\n",
    "    return best_state\n",
    "def evaluate_test(best_state):\n",
    "    print(\"\\n======= Test Evaluation =======\")\n",
    "\n",
    "    test_dataset = PCOSMILDataset(test_df, data_root_dir, transform=val_tf)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)\n",
    "\n",
    "    model = TransformerMIL(num_classes=num_classes).to(device)\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    logits_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs = batch[\"images\"].to(device)\n",
    "            label = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "\n",
    "            preds.append(torch.argmax(logits, dim=1).cpu().item())\n",
    "            trues.append(label.cpu().item())\n",
    "            logits_all.append(logits.cpu().numpy())\n",
    "\n",
    "    logits_all = np.vstack(logits_all)\n",
    "    metrics = compute_metrics(trues, preds, logits_all)\n",
    "    print(\"Test Metrics:\", metrics)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba76ea9",
   "metadata": {},
   "source": [
    "## Vision Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c3816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def zero_shot_classification(\n",
    "    img_root_dir,\n",
    "    data_path,\n",
    "    save_root_dir,\n",
    "    model_ckpt=\"google/siglip2-so400m-patch14-384\",\n",
    "    batch_size=16,\n",
    "    labels=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform zero-shot image classification and save the top-1 results as a CSV.\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        labels = [\"Benign\", \"Borderline\", \"Malignant\"]\n",
    "    label_to_num = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "    os.makedirs(save_root_dir, exist_ok=True)\n",
    "\n",
    "    # Set up the Huggingface pipeline\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    classifier = pipeline(\n",
    "        model=model_ckpt,\n",
    "        task=\"zero-shot-image-classification\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    data_df = pd.read_csv(data_path)\n",
    "    img_files = data_df['filename'].astype(str).tolist()\n",
    "    gt_labels = data_df['USG_Ontology'].tolist()\n",
    "\n",
    "    top1_results = {\n",
    "        \"filename\": [],\n",
    "        \"label\": [],\n",
    "        \"preds\": [],\n",
    "        \"probs\": [],\n",
    "    }\n",
    "\n",
    "    # Inference in batches\n",
    "    for start_idx in range(0, len(img_files), batch_size):\n",
    "        batch_imgs = img_files[start_idx : start_idx + batch_size]\n",
    "        batch_img_paths = [os.path.join(img_root_dir, f\"{img_file}.png\") for img_file in batch_imgs]\n",
    "\n",
    "        outputs = classifier(batch_img_paths, candidate_labels=labels)\n",
    "        for i, out in enumerate(outputs):\n",
    "            # Get the highest scoring label\n",
    "            top_result = max(out, key=lambda x: x[\"score\"])\n",
    "            top1_results[\"filename\"].append(batch_imgs[i])\n",
    "            top1_results[\"label\"].append(gt_labels[start_idx + i])\n",
    "            top1_results[\"preds\"].append(label_to_num[top_result[\"label\"]])\n",
    "            top1_results[\"probs\"].append(top_result[\"score\"])\n",
    "\n",
    "    # Ensure lengths are consistent before saving\n",
    "    min_len = min(\n",
    "        len(top1_results[\"filename\"]),\n",
    "        len(top1_results[\"label\"]),\n",
    "        len(top1_results[\"preds\"]),\n",
    "        len(top1_results[\"probs\"])\n",
    "    )\n",
    "    df = pd.DataFrame({k: v[:min_len] for k, v in top1_results.items()})\n",
    "    csv_path = os.path.join(save_root_dir, \"top1_results.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"[INFO] Results saved to {csv_path}\")\n",
    "\n",
    "# 사용 예시\n",
    "IMG_ROOT_DIR = \"/workspace/pcos_dataset/Dataset\"\n",
    "DATA_PATH = \"/workspace/pcos_dataset/labels/통합_Dataset_info.csv\"\n",
    "CKPT = \"google/siglip2-so400m-patch14-384\"\n",
    "SAVE_ROOT_DIR = f\"/workspace/pcos_dataset/results/zero_shot/siglip2/{CKPT}\"\n",
    "\n",
    "zero_shot_classification(\n",
    "    img_root_dir=IMG_ROOT_DIR,\n",
    "    data_path=DATA_PATH,\n",
    "    save_root_dir=SAVE_ROOT_DIR,\n",
    "    model_ckpt=CKPT,\n",
    "    batch_size=16,\n",
    "    labels=[\"Benign\", \"Borderline\", \"Malignant\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603d2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dce532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
