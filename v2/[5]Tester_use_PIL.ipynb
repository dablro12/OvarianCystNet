{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed Fix: 42\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from lib.seed import seed_prefix \n",
    "import sys, os \n",
    "load_dotenv()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "seed_prefix(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1]-[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label|0:양성, 1:중간형, 2:악성</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_R009_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>R009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_R014_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>R014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_R017_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>R017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_R019_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>R019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_R028_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>R028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>2_R893_00001</td>\n",
       "      <td>2</td>\n",
       "      <td>R893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>2_R893_00002</td>\n",
       "      <td>2</td>\n",
       "      <td>R893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2_R893_00003</td>\n",
       "      <td>2</td>\n",
       "      <td>R893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2_R919_00001</td>\n",
       "      <td>2</td>\n",
       "      <td>R919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>2_R919_00002</td>\n",
       "      <td>2</td>\n",
       "      <td>R919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename  label|0:양성, 1:중간형, 2:악성   PID\n",
       "0    0_R009_00001                        0  R009\n",
       "1    0_R014_00001                        0  R014\n",
       "2    0_R017_00001                        0  R017\n",
       "3    0_R019_00001                        0  R019\n",
       "4    0_R028_00001                        0  R028\n",
       "..            ...                      ...   ...\n",
       "587  2_R893_00001                        2  R893\n",
       "588  2_R893_00002                        2  R893\n",
       "589  2_R893_00003                        2  R893\n",
       "590  2_R919_00001                        2  R919\n",
       "591  2_R919_00002                        2  R919\n",
       "\n",
       "[592 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.dataset import data_split\n",
    "import pandas as pd \n",
    "data_df = pd.read_csv(os.getenv('DATASHEET_PATH'))\n",
    "data_dir = os.getenv('DATA_DIR')\n",
    "train_df, test_df = data_split(data_df, split_num = 5)\n",
    "test_df.to_csv('/home/eiden/eiden/PCOS-roi-classification/v2/data/datasheet_test.csv', encoding = 'utf-8-sig', index = False)\n",
    "binary_use = False\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.dataset import PCOS_Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "labels = test_df['label|0:양성, 1:중간형, 2:악성']\n",
    "filenames = test_df['filename']\n",
    "filepaths = [os.path.join(data_dir, filename) for filename in filenames]\n",
    "filepaths\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.Grayscale(num_output_channels = 3),\n",
    "    T.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def reshape_transform(filepath, label, transform):\n",
    "    data = Image.open(filepath + '.png')\n",
    "    if binary_use:\n",
    "        label = 0 if label == 1 else 1 # 보더라인을 양성에 붙힌 경우 AUC : 0.7246253000224796\n",
    "        # 다중 뷴류 문제라면 float 타입을 쓰는 경우가 많습니다. (원-핫이 아닌 class index라고 가정)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "    else:\n",
    "        # 다중 뷴류 문제라면 long 타입을 쓰는 경우가 많습니다. (원-핫이 아닌 class index라고 가정)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "    if transform:\n",
    "        data = transform(data)\n",
    "    return data, label\n",
    "\n",
    "X, label = reshape_transform(filepaths[0], labels[0], transform)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X.permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eiden/miniconda3/envs/pcos/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Large_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from models import Model_Loader\n",
    "model = Model_Loader(model_name = 'convnext' + '_' + 'l', num_classes = 3).to('cuda')\n",
    "target_layers = [model.features[-1][-1].block] # ConvNext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./log/2025-03-04_10-39-40_fold1.pth',\n",
       " './log/2025-03-04_10-39-40_fold2.pth',\n",
       " './log/2025-03-04_10-39-40_fold3.pth',\n",
       " './log/2025-03-04_10-39-40_fold4.pth',\n",
       " './log/2025-03-04_10-39-40_fold5.pth']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import get_checkpoint_path\n",
    "ckpt_paths = get_checkpoint_path(\n",
    "    checkpoint_dir = './log/',\n",
    "    datetime = \"2025-03-04_10-39-40\"\n",
    ")\n",
    "ckpt_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize, Loss 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Settings] Train - Hyper Parmas 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from metric import calculate_metrics, plot_confusion_matrix_from_preds\n",
    "from lib.pytorch_grad_cam import (\n",
    "    GradCAM,\n",
    "    HiResCAM,\n",
    "    ScoreCAM,\n",
    "    GradCAMPlusPlus,\n",
    "    AblationCAM,\n",
    "    XGradCAM,\n",
    "    EigenCAM,\n",
    "    FullGrad\n",
    ")\n",
    "\n",
    "from lib.pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from lib.pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from visualize import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with K-Fold Grad CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold_1: 100%|██████████| 592/592 [06:30<00:00,  1.52it/s]\n",
      "Fold_2: 100%|██████████| 592/592 [06:54<00:00,  1.43it/s]\n",
      "Fold_3: 100%|██████████| 592/592 [06:54<00:00,  1.43it/s]\n",
      "Fold_4:  29%|██▉       | 173/592 [04:12<10:12,  1.46s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m cam\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# targets=[ClassifierOutputTarget(C의 class index)]\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# targets=None이면 classification score가 가장 높은 클래스에 대한 결과를 보여줌 -> Pred값 보여줌\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mClassifierOutputTarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# 특정 class C에 대한 결과를 확인하려면 아래와 같이 설정\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Flipping을 통해 실행시간이 x6으로 늘어나서 물체를 잘 보여주도록 함. \u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43maug_smooth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#  # Noise를 제거하여 물체를 잘 보여주도록 함.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m y_res_val \u001b[38;5;241m=\u001b[39m cam\u001b[38;5;241m.\u001b[39moutputs\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m# 모델 출력 1 x Num Classes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(y_res_val, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 확률 값 : 1 x Num Classes\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:184\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    177\u001b[0m     input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Smooth the CAM result with test time augmentation\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_augmentation_smoothing\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_tensor, targets, eigen_smooth)\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:160\u001b[0m, in \u001b[0;36mBaseCAM.forward_augmentation_smoothing\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtta_transforms:\n\u001b[1;32m    159\u001b[0m     augmented_tensor \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39maugment_image(input_tensor)\n\u001b[0;32m--> 160\u001b[0m     cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The ttach library expects a tensor of size BxCxHxW\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     cam \u001b[38;5;241m=\u001b[39m cam[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:110\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     99\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:141\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[1;32m    139\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 141\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    143\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/base_cam.py:77\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid activation shape. Get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(activations\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eigen_smooth:\n\u001b[0;32m---> 77\u001b[0m     cam \u001b[38;5;241m=\u001b[39m \u001b[43mget_2d_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweighted_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     cam \u001b[38;5;241m=\u001b[39m weighted_activations\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/pytorch_grad_cam/utils/svd_on_activations.py:16\u001b[0m, in \u001b[0;36mget_2d_projection\u001b[0;34m(activation_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Centering before the SVD seems to be important here,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Otherwise the image returned is negative\u001b[39;00m\n\u001b[1;32m     14\u001b[0m reshaped_activations \u001b[38;5;241m=\u001b[39m reshaped_activations \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     15\u001b[0m     reshaped_activations\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m U, S, VT \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_matrices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m projection \u001b[38;5;241m=\u001b[39m reshaped_activations \u001b[38;5;241m@\u001b[39m VT[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     18\u001b[0m projection \u001b[38;5;241m=\u001b[39m projection\u001b[38;5;241m.\u001b[39mreshape(activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/pcos/lib/python3.11/site-packages/numpy/linalg/_linalg.py:1812\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1808\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->DdD\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->ddd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call\u001b[38;5;241m=\u001b[39m_raise_linalgerror_svd_nonconvergence,\n\u001b[1;32m   1810\u001b[0m               invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m, over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m, divide\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1811\u001b[0m               under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m-> 1812\u001b[0m     u, s, vh \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m u \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1814\u001b[0m s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mastype(_realType(result_t), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm \n",
    "metric_dict = dict()\n",
    "\n",
    "for fold_num, ckpt_path in enumerate(ckpt_paths):\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    val_prob = []\n",
    "    val_label = []\n",
    "    \n",
    "    for filepath, label in tqdm(zip(filepaths, labels), total = len(filepaths), desc = f'Fold_{fold_num+1}'):\n",
    "        with GradCAM(model = model, target_layers = target_layers) as cam:\n",
    "            X, label = reshape_transform(filepath, label, transform)\n",
    "            X, label = X.to('cuda'), label.to('cuda')\n",
    "            if len(X.shape) == 3: # (C, H, W) -> (B, C, H, W)\n",
    "                X = X.unsqueeze(0).to('cuda')\n",
    "            \n",
    "            cam.batch_size = X.shape[0]\n",
    "            grayscale_cam = cam(\n",
    "                input_tensor = X.to('cuda'), \n",
    "                # targets=[ClassifierOutputTarget(C의 class index)]\n",
    "                # targets=None이면 classification score가 가장 높은 클래스에 대한 결과를 보여줌 -> Pred값 보여줌\n",
    "                targets = [ClassifierOutputTarget(label)],       # 특정 class C에 대한 결과를 확인하려면 아래와 같이 설정\n",
    "                eigen_smooth = True, # Flipping을 통해 실행시간이 x6으로 늘어나서 물체를 잘 보여주도록 함. \n",
    "                aug_smooth = True, #  # Noise를 제거하여 물체를 잘 보여주도록 함.\n",
    "                )\n",
    "\n",
    "            y_res_val = cam.outputs.detach() # 모델 출력 1 x Num Classes\n",
    "            if binary_use:\n",
    "                y_pred = torch.sigmoid(y_res_val).cpu()\n",
    "                \n",
    "            else:\n",
    "                y_pred = F.softmax(y_res_val, dim = 1) # 확률 값 : 1 x Num Classes\n",
    "                \n",
    "            val_prob.append(y_pred.cpu().detach())\n",
    "            val_label.append(label.cpu().detach().unsqueeze(0))\n",
    "            \n",
    "            ##%% Gray Scale CAM\n",
    "            # 만약 grayscale_cam[i]도 torch.Tensor라면 numpy로 변환 (이미 2D여야 함)\n",
    "            cam_mask = grayscale_cam.squeeze(0).cpu().numpy() if torch.is_tensor(grayscale_cam.squeeze(0)) else grayscale_cam.squeeze(0) # model output shape\n",
    "            # img를 numpy로 변환 & scailing img to 0~1\n",
    "            img = np.asarray(Image.open(filepath + '.png').convert('RGB')) / 255.0\n",
    "            # cam_mask size scaling to img size\n",
    "            cam_mask = cv2.resize(cam_mask, (img.shape[1], img.shape[0]))\n",
    "            \n",
    "            visualization = show_cam_on_image(img, cam_mask, use_rgb=True, image_weight = 0.6)\n",
    "            \n",
    "            ## save to image\n",
    "            save_dir = ckpt_path.replace('.pth', '')\n",
    "            os.makedirs(save_dir, exist_ok = True)\n",
    "            save_path = os.path.join(save_dir, filepath.split('/')[-1] + '.png')\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(visualization, cv2.COLOR_RGB2BGR))\n",
    "            # 각 파일 처리 후 사용한 메모리 해제\n",
    "            del X, grayscale_cam, y_res_val, y_pred\n",
    "            torch.cuda.empty_cache()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "        \n",
    "    #%% Validation AUC 계산\n",
    "    val_pred = torch.cat(val_prob, dim = 0)\n",
    "    val_label = torch.cat(val_label, dim = 0)\n",
    "    metric_dict[f'Fold_{fold_num+1}'] = calculate_metrics(labels = val_label, preds = val_pred, binary_use = False)\n",
    "\n",
    "    class_names = ['양성', '중간형', '악성']\n",
    "    metric_dict[f'Fold_{fold_num+1}']['Best Threshold'] = plot_roc_curve(true_labels = val_label, pred_probs = val_pred, binary_use = binary_use, class_names = class_names)\n",
    "    plot_confusion_matrix_from_preds(val_label, val_pred, binary_use=False, class_names=class_names, save_path = ckpt_path.replace('.pth' ,'_CM.png'), normalize=True)\n",
    "    # 폴드 처리 후도 캐시 비우기\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "metric_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcos",
   "language": "python",
   "name": "pcos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
