{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Weight Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "from thop import profile\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import copy  # deepcopy를 사용하기 위해 추가\n",
    "\n",
    "sys.path.append('../')\n",
    "from model.loader import model_Loader  # 필요에 따라 수정\n",
    "\n",
    "ckpt_dir = '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint'\n",
    "\n",
    "model_dict = {\n",
    "    'resnet' : models.resnet34(weights = models.ResNet34_Weights.IMAGENET1K_V1),\n",
    "    'mobilenet' : models.mobilenet_v3_large(weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V2),\n",
    "    'efficient' : models.efficientnet_v2_l(weights = models.EfficientNet_V2_L_Weights.IMAGENET1K_V1),\n",
    "    'convnext' : models.convnext_large(weights = models.ConvNeXt_Large_Weights.IMAGENET1K_V1), \n",
    "    'swin-transformer' : models.swin_v2_b(weights = models.Swin_V2_B_Weights.IMAGENET1K_V1),  \n",
    "    'vision-transformer' : models.vit_l_16(weights = models.ViT_L_16_Weights.IMAGENET1K_V1),\n",
    "    'maxvit' : models.maxvit_t(weights = models.MaxVit_T_Weights.IMAGENET1K_V1)\n",
    "}\n",
    "\n",
    "# 초기 딕셔너리 설정\n",
    "params_dict = {model_name: 0 for model_name in model_dict.keys()}\n",
    "inference_dict = params_dict.copy()\n",
    "inference_time_dict = params_dict.copy()\n",
    "flops_dict = {}\n",
    "memory_dict = {}\n",
    "model_size_dict = {}\n",
    "throughput_dict = {}\n",
    "layer_count_dict = {}\n",
    "accuracy_dict = {}\n",
    "\n",
    "def params_count(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def inference_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def measure_inference_time(model, input_tensor, device='cpu', num_runs=100):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 워밍업\n",
    "        for _ in range(10):\n",
    "            _ = model(input_tensor)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            _ = model(input_tensor)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time = total_time / num_runs\n",
    "    return avg_time\n",
    "\n",
    "def calculate_flops(model, input_tensor):\n",
    "    # 모델의 복사본을 생성하고 CPU로 이동\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    # 입력 텐서를 CPU로 이동\n",
    "    input_cpu = input_tensor.cpu()\n",
    "    model_cpu.eval()\n",
    "    with torch.no_grad():\n",
    "        flops, params = profile(model_cpu, inputs=(input_cpu,))\n",
    "    return flops\n",
    "\n",
    "def measure_memory_usage(model, input_tensor, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(input_tensor)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        mem = torch.cuda.max_memory_allocated(device) / (1024 ** 2)  # MB 단위\n",
    "    else:\n",
    "        mem = 0  # CPU 메모리 사용량 측정은 별도로 필요\n",
    "    \n",
    "    return mem\n",
    "\n",
    "def get_model_size(model):\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        torch.save(model.state_dict(), tmp.name)\n",
    "        tmp.seek(0, os.SEEK_END)\n",
    "        size = tmp.tell() / (1024 ** 2)  # MB 단위\n",
    "    return size\n",
    "\n",
    "def count_layers(model):\n",
    "    return len(list(model.modules())) - 1  # 최상위 모델 제외\n",
    "\n",
    "def calculate_throughput(avg_inference_time, batch_size=1):\n",
    "    return batch_size / avg_inference_time\n",
    "\n",
    "# 입력 데이터 준비\n",
    "input_size = (1, 3, 224, 224)\n",
    "input_tensor = torch.randn(input_size)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 정확도 측정을 위한 데이터셋 준비 (예: CIFAR-10)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 모든 모델에 대해 지표 측정\n",
    "for model_name, model in model_dict.items():\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    # 파라미터 수 측정\n",
    "    params_dict[model_name] = params_count(model)\n",
    "    \n",
    "    # 인퍼런스 가능 파라미터 수 측정\n",
    "    inference_dict[model_name] = inference_count(model)\n",
    "    \n",
    "    # 인퍼런스 시간 측정\n",
    "    avg_inference_time = measure_inference_time(model, input_tensor, device=device, num_runs=100)\n",
    "    inference_time_dict[model_name] = avg_inference_time\n",
    "    print(f\"{model_name}: Average Inference Time = {avg_inference_time:.6f} seconds\")\n",
    "    \n",
    "    # FLOPs 측정\n",
    "    try:\n",
    "        flops = calculate_flops(model, input_tensor)\n",
    "        flops_dict[model_name] = flops\n",
    "        print(f\"{model_name}: FLOPs = {flops}\")\n",
    "    except Exception as e:\n",
    "        flops_dict[model_name] = None\n",
    "        print(f\"{model_name}: FLOPs 측정 중 오류 발생 - {e}\")\n",
    "    \n",
    "    # 메모리 사용량 측정\n",
    "    mem = measure_memory_usage(model, input_tensor, device=device)\n",
    "    memory_dict[model_name] = mem\n",
    "    print(f\"{model_name}: Memory Usage = {mem} MB\")\n",
    "    \n",
    "    # 모델 크기 측정\n",
    "    size = get_model_size(model)\n",
    "    model_size_dict[model_name] = size\n",
    "    print(f\"{model_name}: Model Size = {size:.2f} MB\")\n",
    "    \n",
    "    # Throughput 측정\n",
    "    throughput = calculate_throughput(avg_inference_time, batch_size=1)\n",
    "    throughput_dict[model_name] = throughput\n",
    "    print(f\"{model_name}: Throughput = {throughput:.2f} samples/sec\")\n",
    "    \n",
    "    # 레이어 수 측정\n",
    "    layers = count_layers(model)\n",
    "    layer_count_dict[model_name] = layers\n",
    "    print(f\"{model_name}: Number of Layers = {layers}\")\n",
    "    \n",
    "    print(\"-\" * 50)  # 구분선 추가\n",
    "\n",
    "# DataFrame으로 변환\n",
    "params_df = pd.DataFrame(params_dict.items(), columns=['model', 'Parameters'])\n",
    "inference_df = pd.DataFrame(inference_dict.items(), columns=['model', 'Trainable Parameters'])\n",
    "inference_time_df = pd.DataFrame(inference_time_dict.items(), columns=['model', 'Avg Inference Time (s)'])\n",
    "flops_df = pd.DataFrame(flops_dict.items(), columns=['model', 'FLOPs'])\n",
    "memory_df = pd.DataFrame(memory_dict.items(), columns=['model', 'Memory Usage (MB)'])\n",
    "model_size_df = pd.DataFrame(model_size_dict.items(), columns=['model', 'Model Size (MB)'])\n",
    "throughput_df = pd.DataFrame(throughput_dict.items(), columns=['model', 'Throughput (samples/sec)'])\n",
    "layers_df = pd.DataFrame(layer_count_dict.items(), columns=['model', 'Number of Layers'])\n",
    "\n",
    "# 모든 데이터프레임 합치기\n",
    "result_df = params_df.merge(inference_df, on='model') \\\n",
    "                     .merge(inference_time_df, on='model') \\\n",
    "                     .merge(flops_df, on='model') \\\n",
    "                     .merge(memory_df, on='model') \\\n",
    "                     .merge(model_size_df, on='model') \\\n",
    "                     .merge(throughput_df, on='model') \\\n",
    "                     .merge(layers_df, on='model')\n",
    "\n",
    "# 인덱스를 모델 이름으로 설정\n",
    "result_df.set_index('model', inplace=True)\n",
    "\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics/model_params.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 제공된 LaTeX 코드 (소수점 3자리로 수정됨)\n",
    "latex_code = r\"\"\"\n",
    "\\documentclass{standalone}\n",
    "\\usepackage{booktabs}\n",
    "\\usepackage{siunitx}\n",
    "\n",
    "\\sisetup{\n",
    "    round-mode          = places,\n",
    "    round-precision     = 3,\n",
    "}\n",
    "\n",
    "\\begin{document}\n",
    "\\begin{table}[htbp]\n",
    "    \\centering\n",
    "    \\caption{Model Metrics Comparison}\n",
    "    \\label{tab:model_metrics}\n",
    "    \\begin{tabular}{l\n",
    "                    S[table-format=8.0]\n",
    "                    S[table-format=8.0]\n",
    "                    S[table-format=1.3]\n",
    "                    S[table-format=10.0]\n",
    "                    S[table-format=4.3]\n",
    "                    S[table-format=5.2]\n",
    "                    S[table-format=8.3]\n",
    "                    S[table-format=3.0]}\n",
    "        \\toprule\n",
    "        \\textbf{Model} & \\textbf{Parameters} & \\textbf{Trainable Parameters} & \\textbf{Avg Inference Time (s)} & \\textbf{FLOPs} & \\textbf{Memory Usage (MB)} & \\textbf{Model Size (MB)} & \\textbf{Throughput (samples/sec)} & \\textbf{Number of Layers} \\\\\n",
    "        \\midrule\n",
    "        resnet & 21797672 & 21797672 & 0.003 & 3678739456 & 103.312 & 83.286 & 364.821 & 115 \\\\\n",
    "        mobilenet & 5483032 & 5483032 & 0.003 & 234838456 & 244.871 & 21.113 & 331.254 & 254 \\\\\n",
    "        efficient & 118515272 & 118515272 & 0.017 & 12379301696 & 798.776 & 454.599 & 58.461 & 1408 \\\\\n",
    "        convnext & 197767336 & 197767336 & 0.004 & 34389508608 & 1458.106 & 754.544 & 220.788 & 382 \\\\\n",
    "        swin-transformer & 87930848 & 87930848 & 0.014 & 10270570496 & 1780.294 & 336.377 & 69.733 & 431 \\\\\n",
    "        vision-transformer & 304326632 & 304326632 & 0.006 & 39856041984 & 2929.969 & 1161.028 & 159.113 & 295 \\\\\n",
    "        maxvit & 30919624 & 30919624 & 0.012 & 5456315648 & 3176.244 & 118.776 & 83.416 & 669 \\\\\n",
    "        \\bottomrule\n",
    "    \\end{tabular}\n",
    "\\end{table}\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "# 출력 경로 설정\n",
    "output_dir = \"/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics/\"\n",
    "output_svg_path = os.path.join(output_dir, \"model_metrics.svg\")\n",
    "\n",
    "# 임시 디렉토리 생성 (출력 디렉토리와 분리)\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    print(f\"임시 디렉토리 생성: {tmpdirname}\")\n",
    "    \n",
    "    # LaTeX 파일 작성\n",
    "    tex_file = os.path.join(tmpdirname, \"model_metrics.tex\")\n",
    "    with open(tex_file, \"w\") as f:\n",
    "        f.write(latex_code)\n",
    "    print(\"LaTeX 파일 작성 완료.\")\n",
    "    \n",
    "    # LaTeX 컴파일 (PDF 생성)\n",
    "    try:\n",
    "        print(\"LaTeX 파일을 PDF로 컴파일 중...\")\n",
    "        subprocess.run(\n",
    "            [\"pdflatex\", \"-interaction=nonstopmode\", \"model_metrics.tex\"],\n",
    "            cwd=tmpdirname,\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        print(\"PDF 컴파일 완료.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"LaTeX 컴파일 중 오류 발생:\")\n",
    "        print(e.stdout.decode())\n",
    "        print(e.stderr.decode())\n",
    "        exit(1)\n",
    "    \n",
    "    # PDF 파일 경로 확인\n",
    "    pdf_file = os.path.join(tmpdirname, \"model_metrics.pdf\")\n",
    "    if not os.path.exists(pdf_file):\n",
    "        print(\"PDF 파일이 생성되지 않았습니다.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # SVG 변환 (pdf2svg 또는 Inkscape 사용)\n",
    "    svg_file = os.path.join(tmpdirname, \"model_metrics.svg\")\n",
    "    try:\n",
    "        # pdf2svg가 설치되어 있는지 확인\n",
    "        if shutil.which(\"pdf2svg\"):\n",
    "            print(\"pdf2svg를 사용하여 PDF를 SVG로 변환 중...\")\n",
    "            subprocess.run(\n",
    "                [\"pdf2svg\", \"model_metrics.pdf\", \"model_metrics.svg\"],\n",
    "                cwd=tmpdirname,\n",
    "                check=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE\n",
    "            )\n",
    "            print(\"SVG 변환 완료 (pdf2svg 사용).\")\n",
    "        elif shutil.which(\"inkscape\"):\n",
    "            # pdf2svg가 없으면 Inkscape 사용\n",
    "            print(\"pdf2svg가 설치되어 있지 않습니다. Inkscape를 사용하여 변환 중...\")\n",
    "            subprocess.run(\n",
    "                [\"inkscape\", \"model_metrics.pdf\", \"--export-type=svg\", \"--export-filename=model_metrics.svg\"],\n",
    "                cwd=tmpdirname,\n",
    "                check=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE\n",
    "            )\n",
    "            print(\"SVG 변환 완료 (Inkscape 사용).\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"pdf2svg와 Inkscape가 모두 설치되어 있지 않습니다.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"PDF를 SVG로 변환하는 중 오류 발생:\")\n",
    "        print(e.stdout.decode())\n",
    "        print(e.stderr.decode())\n",
    "        exit(1)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        exit(1)\n",
    "    \n",
    "    # SVG 파일 경로 확인\n",
    "    if not os.path.exists(svg_file):\n",
    "        print(\"SVG 파일이 생성되지 않았습니다.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 출력 디렉토리 생성 (없으면)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # SVG 파일 이동\n",
    "    shutil.move(svg_file, output_svg_path)\n",
    "    print(f\"SVG 파일이 성공적으로 저장되었습니다: {output_svg_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "# Add parent directory to sys.path for module imports\n",
    "sys.path.append('../')\n",
    "import warnings\n",
    "import torch\n",
    "from lib.seed import set_seed\n",
    "from lib.evaluation.eval import dataloader_builder, evaluation\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchvision\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "ROOT_DIR = '/mnt/hdd/octc/PCOS_Dataset'\n",
    "SAVE_METRIC_DIR = '/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics'\n",
    "FOLD_NUM = 5\n",
    "OUTER_LAYER_NUM = 3\n",
    "\n",
    "# Initialize\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define evaluation configurations\n",
    "evaluation_configs = [\n",
    "    # {\n",
    "    #     \"description\": \"origin\",\n",
    "    #     \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "    #     \"mask_use\": False,\n",
    "    #     \"model_suffix\": \"origin\",\n",
    "    #     \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "    #     \"data_type\" : \"Dataset\"\n",
    "    \n",
    "    # },\n",
    "    # {\n",
    "    #     \"description\": \"mask\",\n",
    "    #     \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "    #     \"mask_use\": True,\n",
    "    #     \"model_suffix\": \"mask\",\n",
    "    #     \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "    #     \"data_type\" : \"Dataset\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"description\": \"maskwithaugment\",\n",
    "    #     \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "    #     \"mask_use\": True,\n",
    "    #     \"model_suffix\": \"maskwithaugment\",\n",
    "    #     \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "    #     \"data_type\" : \"Dataset\"\n",
    "    # },\n",
    "    {\n",
    "        \"description\": \"originwithaugment\",\n",
    "        \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "        \"mask_use\": False,\n",
    "        \"model_suffix\": \"originwithaugment\",\n",
    "        \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "        \"data_type\" : \"Dataset\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"description\": \"inpaint\",\n",
    "    #     \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "    #     \"mask_use\": False,\n",
    "    #     \"model_suffix\": \"inpaint\",\n",
    "    #     \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "    #     \"data_type\" : \"Dataset_OCI\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"description\": \"inpaintwithaugment\",\n",
    "    #     \"csv_path\": os.path.join(ROOT_DIR, 'test.csv'),\n",
    "    #     \"mask_use\": False,\n",
    "    #     \"model_suffix\": \"inpaintwithaugment\",\n",
    "    #     \"checkpoint_root_dir\" : '/mnt/hdd/octc/PCOS_experiment/multi_v3/checkpoint',\n",
    "    #     \"data_type\" : \"Dataset_OCI\"\n",
    "    # },\n",
    "]\n",
    "\n",
    "def run_evaluation(config):\n",
    "    test_loader = dataloader_builder(\n",
    "        test_csv_path=config[\"csv_path\"],\n",
    "        root_dir=ROOT_DIR,\n",
    "        input_res=(3, 224, 224),\n",
    "        bs_size=32,\n",
    "        mask_use=config[\"mask_use\"],\n",
    "        class_num=OUTER_LAYER_NUM,\n",
    "        data_type = config[\"data_type\"]\n",
    "        \n",
    "    )\n",
    "    filtered_model_cards = [\n",
    "        card for card in sorted(os.listdir(config[\"checkpoint_root_dir\"]))\n",
    "        if card.split('-')[-1] == config[\"model_suffix\"]\n",
    "    ]\n",
    "    # for i in range(len(filtered_model_cards)):\n",
    "    #     print(filtered_model_cards[i])\n",
    "    num_models = len(filtered_model_cards) // FOLD_NUM\n",
    "    print(f\"Number of models: {num_models}\")\n",
    "    evaluation(\n",
    "        outlayer_num=OUTER_LAYER_NUM,\n",
    "        model_cards=filtered_model_cards,\n",
    "        checkpoint_root_dir=config[\"checkpoint_root_dir\"],\n",
    "        mask_use=config[\"mask_use\"],\n",
    "        device=DEVICE,\n",
    "        test_loader=test_loader,\n",
    "        save_metric_dir=SAVE_METRIC_DIR\n",
    "    )\n",
    "    \n",
    "\n",
    "# Execute evaluations\n",
    "for config in evaluation_configs:\n",
    "    run_evaluation(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 메트릭 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "def get_significance_stars(p_value):\n",
    "    \"\"\"Return significance stars based on p-value.\"\"\"\n",
    "    if p_value < 1e-4:\n",
    "        return '****'\n",
    "    elif p_value < 1e-3:\n",
    "        return '***'\n",
    "    elif p_value < 1e-2:\n",
    "        return '**'\n",
    "    elif p_value < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'  # not significant\n",
    "\n",
    "def visualize_box_plot_with_p_value(df):\n",
    "    \"\"\"\n",
    "    Visualize box plots for each metric grouped by version with annotated p-values.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing metrics and a 'Version' column.\n",
    "    \"\"\"\n",
    "    import itertools  # Moved inside the function for self-containment\n",
    "\n",
    "    # Required metrics\n",
    "    metrics = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC']\n",
    "\n",
    "    # Ensure all required columns exist\n",
    "    for metric in metrics + ['Version']:\n",
    "        if metric not in df.columns:\n",
    "            raise ValueError(f\"Column '{metric}' not found in the DataFrame.\")\n",
    "\n",
    "    # Convert metric columns to numeric, coercing errors to NaN\n",
    "    df[metrics] = df[metrics].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Drop rows with NaN in any of the metric columns or 'Version'\n",
    "    df = df.dropna(subset=metrics + ['Version'])\n",
    "\n",
    "    # Get unique versions in reverse order\n",
    "    versions = sorted(df['Version'].unique(), reverse=True)\n",
    "    num_versions = len(versions)\n",
    "\n",
    "    if num_versions < 2:\n",
    "        raise ValueError(\"At least two unique versions are required for comparison.\")\n",
    "\n",
    "    # Define subplot grid based on number of metrics\n",
    "    cols = 3\n",
    "    rows = int(np.ceil(len(metrics) / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Prepare data for boxplot\n",
    "        data = [df[df['Version'] == version][metric].dropna().values for version in versions]\n",
    "\n",
    "        # Create box plot\n",
    "        bp = ax.boxplot(data, patch_artist=True, labels=versions)\n",
    "\n",
    "        # Customize boxplot appearance\n",
    "        # Define colors for the base version and other versions\n",
    "        base_color = 'skyblue'\n",
    "        other_colors = ['lightgreen', 'salmon', 'gold', 'plum', 'lightgrey']\n",
    "        colors = [base_color] + other_colors[:num_versions - 1]\n",
    "\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xlabel('Version')\n",
    "        ax.set_ylabel(metric)\n",
    "\n",
    "        # Perform t-tests between the first version and others\n",
    "        base_data = data[0]\n",
    "        y_max = max([d.max() for d in data])  # Current maximum y-value for the metric\n",
    "\n",
    "        # Calculate the range for y-axis\n",
    "        y_min = min([d.min() for d in data])\n",
    "        y_range = y_max - y_min\n",
    "        y_offset_initial = y_range * 0.05  # 5% of the range for initial offset\n",
    "        y_offset_increment = y_range * 0.05  # 5% increment for each additional annotation\n",
    "\n",
    "        # Initialize the starting y-position for annotations\n",
    "        current_y = y_max + y_offset_initial\n",
    "\n",
    "        for i in range(1, num_versions):\n",
    "            compare_data = data[i]\n",
    "            t_stat, p_value = ttest_ind(base_data, compare_data, equal_var=False)  # Welch's t-test\n",
    "\n",
    "            significance = get_significance_stars(p_value)\n",
    "\n",
    "            # Coordinates for annotation\n",
    "            x1, x2 = 1, i + 1  # Base version is at position 1\n",
    "            h = y_range * 0.01  # Height of the annotation line\n",
    "            col = 'k'  # Color of the annotation lines and text\n",
    "\n",
    "            # Draw the annotation lines\n",
    "            ax.plot([x1, x1, x2, x2], [current_y, current_y + h, current_y + h, current_y], lw=1.3, c=col)\n",
    "\n",
    "            # Annotate the p-value and significance\n",
    "            # ax.text((x1 + x2) * 0.5, current_y + h, f'p = {p_value:.2e}\\n{significance}', ha='center', va='bottom', fontsize=10)\n",
    "            ax.text((x1 + x2) * 0.5, current_y + h, f'{significance}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "            # Increment the y-position for the next annotation\n",
    "            current_y += y_offset_increment\n",
    "\n",
    "        # Adjust y-axis limit to accommodate all annotations\n",
    "        ax.set_ylim(top=current_y + y_offset_increment)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for idx in range(len(metrics), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# binary_metric_save 함수는 이전에 수정된 버전을 사용합니다.\n",
    "\n",
    "def binary_metric_save(save_metric_dir: str, pickles: list):\n",
    "    # Initialize DataFrame to store results\n",
    "    all_metrics = pd.DataFrame(columns=['Model', 'Version', 'Fold', 'Accuracy', 'Precision',\n",
    "                                        'Sensitivity', 'Specificity', 'F1-score', 'AUC'])\n",
    "    roc_metrics = {}\n",
    "    # Read data from pickle files and add to DataFrame\n",
    "    for idx, pk in enumerate(pickles):\n",
    "        pickle_path = os.path.join(save_metric_dir, pk)\n",
    "        # Use regex to extract model_name, version, fold_num\n",
    "        match = re.match(r'(.+?)_(.+?)_(.+?)\\.pkl$', pk)\n",
    "        \n",
    "        if match:\n",
    "            model_name, version, fold_num = match.groups()\n",
    "        else:\n",
    "            print(f\"Filename {pk} does not match the expected pattern. Skipping.\")\n",
    "            continue  # Skip files that don't match the expected pattern\n",
    "\n",
    "        # Load data from pickle file\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            metric = pickle.load(f)\n",
    "\n",
    "        roc_metrics[f'{model_name}_{version}_{fold_num}'] = {\n",
    "            'fpr': metric['FPR'],\n",
    "            'tpr': metric['TPR'],\n",
    "            'roc_auc': metric['AUC'],\n",
    "            'thresholds': metric['Thresholds']\n",
    "        }\n",
    "        # Convert metric data to DataFrame\n",
    "        metric_df = pd.DataFrame(metric)\n",
    "\n",
    "        # Select required columns and the last row\n",
    "        metric_df = metric_df[['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC']].iloc[-1]\n",
    "\n",
    "        # Add additional info: Model, Version, Fold\n",
    "        metric_df['Model'] = model_name\n",
    "        metric_df['Version'] = version\n",
    "        metric_df['Fold'] = pd.to_numeric(fold_num, errors='coerce')  # Ensure 'Fold' is numeric\n",
    "\n",
    "        # Reorder columns\n",
    "        metric_df = metric_df[['Model', 'Version', 'Fold', 'Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC']]\n",
    "\n",
    "        # Append to all_metrics DataFrame\n",
    "        all_metrics = pd.concat([all_metrics, metric_df.to_frame().T], ignore_index=True)\n",
    "\n",
    "    # Exclude 'Fold' from mean calculation\n",
    "    avg_metrics = all_metrics.drop(columns=['Fold']).groupby(['Model', 'Version'], as_index=False).mean()\n",
    "\n",
    "    # Sort by AUC in descending order\n",
    "    avg_metrics = avg_metrics.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "    # Round numeric columns to 3 decimal places\n",
    "    numeric_cols = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC']\n",
    "    avg_metrics[numeric_cols] = avg_metrics[numeric_cols].apply(pd.to_numeric, errors='coerce').round(3)\n",
    "\n",
    "    return roc_metrics, all_metrics, avg_metrics\n",
    "\n",
    "# 저장된 metric 파일이 있는 디렉토리 경로\n",
    "save_metric_dir = '/mnt/hdd/octc/PCOS_experiment/binary_v2/metrics/'\n",
    "# 디렉토리 내 모든 pickle 파일 목록을 정렬\n",
    "pickles = sorted(os.listdir(save_metric_dir))\n",
    "\n",
    "# metrics 데이터를 불러오는 함수\n",
    "roc_metrics, all_metrics, avg_metrics = binary_metric_save(save_metric_dir, pickles)\n",
    "avg_metrics.to_csv('/mnt/hdd/octc/PCOS_experiment/binary_v2/all_metrics.csv', index=False)\n",
    "\n",
    "# 예제 사용\n",
    "visualize_box_plot_with_p_value(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def multi_class_metric_save(save_metric_dir: str, pickles: List[str]) -> Tuple[dict, pd.DataFrame, pd.DataFrame]:\n",
    "    # Initialize DataFrame to store results\n",
    "    all_metrics = pd.DataFrame(columns=['Model', 'Version', 'Fold', 'Accuracy', 'Precision',\n",
    "                                        'Sensitivity', 'Specificity', 'F1-score', 'AUC'])\n",
    "    roc_metrics = {}\n",
    "    \n",
    "    # Read data from pickle files and add to DataFrame\n",
    "    for idx, pk in enumerate(pickles):\n",
    "        pickle_path = os.path.join(save_metric_dir, pk)\n",
    "        \n",
    "        # Use regex to extract model_name, version, fold_num\n",
    "        match = re.match(r'(.+?)_(.+?)_(.+?)\\.pkl$', pk)\n",
    "        \n",
    "        if match:\n",
    "            model_name, version, fold_num = match.groups()\n",
    "            if version == 'transformer':\n",
    "                version = 'mask-ft-medsam1'\n",
    "        else:\n",
    "            print(f\"Filename {pk} does not match the expected pattern. Skipping.\")\n",
    "            continue  # Skip files that don't match the expected pattern\n",
    "\n",
    "        # Load data from pickle file\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            metric = pickle.load(f)\n",
    "        \n",
    "        # Add ROC metrics if available\n",
    "        if 'FPR' in metric and 'TPR' in metric and 'AUC' in metric and 'Thresholds' in metric:\n",
    "            roc_metrics[f'{model_name}_{version}_{fold_num}'] = {\n",
    "                'fpr': metric['FPR'],\n",
    "                'tpr': metric['TPR'],\n",
    "                'roc_auc': metric['AUC'],\n",
    "                'thresholds': metric['Thresholds']\n",
    "            }\n",
    "        \n",
    "        # Convert metric data to DataFrame and check available keys\n",
    "        available_metrics = {key: metric[key] for key in ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC'] if key in metric}\n",
    "        metric_df = pd.DataFrame([available_metrics])\n",
    "\n",
    "        # Add additional info: Model, Version, Fold\n",
    "        metric_df['Model'] = model_name\n",
    "        metric_df['Version'] = version\n",
    "        metric_df['Fold'] = pd.to_numeric(fold_num, errors='coerce')  # Ensure 'Fold' is numeric\n",
    "\n",
    "        # Reorder columns\n",
    "        metric_df = metric_df[['Model', 'Version', 'Fold'] + [col for col in ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC'] if col in metric_df]]\n",
    "\n",
    "        # Append to all_metrics DataFrame\n",
    "        all_metrics = pd.concat([all_metrics, metric_df], ignore_index=True)\n",
    "\n",
    "    # Exclude 'Fold' from mean calculation\n",
    "    avg_metrics = all_metrics.drop(columns=['Fold']).groupby(['Model', 'Version'], as_index=False).mean()\n",
    "\n",
    "    # Sort by AUC in descending order if AUC is available\n",
    "    if 'AUC' in avg_metrics.columns:\n",
    "        avg_metrics = avg_metrics.sort_values(by='AUC', ascending=False)\n",
    "\n",
    "    # Round numeric columns to 3 decimal places\n",
    "    # numeric_cols = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score', 'AUC']\n",
    "    numeric_cols = ['Accuracy', 'Precision', 'Sensitivity', 'Specificity', 'F1-score']\n",
    "    for col in numeric_cols:\n",
    "        if col in avg_metrics.columns:\n",
    "            avg_metrics[col] = avg_metrics[col].round(3)\n",
    "\n",
    "    return roc_metrics, all_metrics, avg_metrics\n",
    "\n",
    "type = \"augment\"\n",
    "# 저장된 metric 파일이 있는 디렉토리 경로\n",
    "save_metric_dir = f'/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics/{type}'\n",
    "\n",
    "# 디렉토리 내 모든 pickle 파일 목록을 정렬\n",
    "pickles = sorted(os.listdir(save_metric_dir))\n",
    "# metrics 데이터를 불러오는 함수\n",
    "roc_metrics, all_metrics, avg_metrics = multi_class_metric_save(save_metric_dir, pickles)\n",
    "\n",
    "# AUC 소수점 3자리까지\n",
    "avg_metrics.sort_values(by='Sensitivity', ascending=False, inplace=True)\n",
    "all_metrics.sort_values(by='Sensitivity', ascending=False, inplace=True)\n",
    "avg_metrics = avg_metrics.round(3)\n",
    "all_metrics = all_metrics.round(3)\n",
    "# # # avg_metrics['AUC'] = avg_metrics['AUC'].round(3)\n",
    "all_metrics.to_csv(f'/mnt/hdd/octc/PCOS_experiment/multi_v3/all_{type}_metrics.csv', index=False)\n",
    "avg_metrics.to_csv(f'/mnt/hdd/octc/PCOS_experiment/multi_v3/avg_{type}_metrics.csv', index=False)\n",
    "visualize_box_plot_with_p_value(df = all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Version</th>\n",
       "      <th>Fold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>convnext</td>\n",
       "      <td>origin</td>\n",
       "      <td>4</td>\n",
       "      <td>75.000</td>\n",
       "      <td>73.299</td>\n",
       "      <td>75.000</td>\n",
       "      <td>79.188</td>\n",
       "      <td>71.860</td>\n",
       "      <td>79.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaintwithaugment</td>\n",
       "      <td>4</td>\n",
       "      <td>74.689</td>\n",
       "      <td>72.368</td>\n",
       "      <td>74.689</td>\n",
       "      <td>78.677</td>\n",
       "      <td>71.649</td>\n",
       "      <td>76.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaint</td>\n",
       "      <td>4</td>\n",
       "      <td>74.689</td>\n",
       "      <td>72.368</td>\n",
       "      <td>74.689</td>\n",
       "      <td>78.677</td>\n",
       "      <td>71.649</td>\n",
       "      <td>76.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaint</td>\n",
       "      <td>3</td>\n",
       "      <td>74.534</td>\n",
       "      <td>73.262</td>\n",
       "      <td>74.534</td>\n",
       "      <td>77.679</td>\n",
       "      <td>71.075</td>\n",
       "      <td>72.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaintwithaugment</td>\n",
       "      <td>3</td>\n",
       "      <td>74.534</td>\n",
       "      <td>73.262</td>\n",
       "      <td>74.534</td>\n",
       "      <td>77.679</td>\n",
       "      <td>71.075</td>\n",
       "      <td>72.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>convnext</td>\n",
       "      <td>origin</td>\n",
       "      <td>3</td>\n",
       "      <td>74.224</td>\n",
       "      <td>71.385</td>\n",
       "      <td>74.224</td>\n",
       "      <td>77.899</td>\n",
       "      <td>70.269</td>\n",
       "      <td>76.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>convnext</td>\n",
       "      <td>origin</td>\n",
       "      <td>2</td>\n",
       "      <td>73.758</td>\n",
       "      <td>72.957</td>\n",
       "      <td>73.758</td>\n",
       "      <td>76.797</td>\n",
       "      <td>69.669</td>\n",
       "      <td>76.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaint</td>\n",
       "      <td>5</td>\n",
       "      <td>73.602</td>\n",
       "      <td>70.174</td>\n",
       "      <td>73.602</td>\n",
       "      <td>78.575</td>\n",
       "      <td>70.458</td>\n",
       "      <td>75.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>convnext</td>\n",
       "      <td>originwithaugment</td>\n",
       "      <td>1</td>\n",
       "      <td>73.602</td>\n",
       "      <td>70.853</td>\n",
       "      <td>73.602</td>\n",
       "      <td>80.007</td>\n",
       "      <td>71.456</td>\n",
       "      <td>75.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaintwithaugment</td>\n",
       "      <td>5</td>\n",
       "      <td>73.602</td>\n",
       "      <td>70.174</td>\n",
       "      <td>73.602</td>\n",
       "      <td>78.575</td>\n",
       "      <td>70.458</td>\n",
       "      <td>75.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>convnext</td>\n",
       "      <td>originwithaugment</td>\n",
       "      <td>2</td>\n",
       "      <td>73.447</td>\n",
       "      <td>71.023</td>\n",
       "      <td>73.447</td>\n",
       "      <td>79.441</td>\n",
       "      <td>71.508</td>\n",
       "      <td>75.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>convnext</td>\n",
       "      <td>mask</td>\n",
       "      <td>4</td>\n",
       "      <td>73.447</td>\n",
       "      <td>69.842</td>\n",
       "      <td>73.447</td>\n",
       "      <td>75.996</td>\n",
       "      <td>68.157</td>\n",
       "      <td>74.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>convnext</td>\n",
       "      <td>originwithaugment</td>\n",
       "      <td>3</td>\n",
       "      <td>73.447</td>\n",
       "      <td>71.191</td>\n",
       "      <td>73.447</td>\n",
       "      <td>78.975</td>\n",
       "      <td>71.305</td>\n",
       "      <td>77.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>convnext</td>\n",
       "      <td>maskwithaugment</td>\n",
       "      <td>4</td>\n",
       "      <td>73.447</td>\n",
       "      <td>69.842</td>\n",
       "      <td>73.447</td>\n",
       "      <td>75.996</td>\n",
       "      <td>68.157</td>\n",
       "      <td>74.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaint</td>\n",
       "      <td>1</td>\n",
       "      <td>73.292</td>\n",
       "      <td>68.994</td>\n",
       "      <td>73.292</td>\n",
       "      <td>76.916</td>\n",
       "      <td>68.852</td>\n",
       "      <td>73.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>convnext</td>\n",
       "      <td>originwithaugment</td>\n",
       "      <td>5</td>\n",
       "      <td>73.292</td>\n",
       "      <td>71.170</td>\n",
       "      <td>73.292</td>\n",
       "      <td>78.936</td>\n",
       "      <td>71.308</td>\n",
       "      <td>78.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaintwithaugment</td>\n",
       "      <td>1</td>\n",
       "      <td>73.292</td>\n",
       "      <td>68.994</td>\n",
       "      <td>73.292</td>\n",
       "      <td>76.916</td>\n",
       "      <td>68.852</td>\n",
       "      <td>73.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>convnext</td>\n",
       "      <td>origin</td>\n",
       "      <td>5</td>\n",
       "      <td>72.826</td>\n",
       "      <td>68.815</td>\n",
       "      <td>72.826</td>\n",
       "      <td>76.726</td>\n",
       "      <td>68.748</td>\n",
       "      <td>78.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>convnext</td>\n",
       "      <td>mask</td>\n",
       "      <td>3</td>\n",
       "      <td>72.826</td>\n",
       "      <td>70.136</td>\n",
       "      <td>72.826</td>\n",
       "      <td>78.097</td>\n",
       "      <td>70.263</td>\n",
       "      <td>74.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>convnext</td>\n",
       "      <td>maskwithaugment</td>\n",
       "      <td>3</td>\n",
       "      <td>72.826</td>\n",
       "      <td>70.136</td>\n",
       "      <td>72.826</td>\n",
       "      <td>78.097</td>\n",
       "      <td>70.263</td>\n",
       "      <td>74.832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>convnext</td>\n",
       "      <td>maskwithaugment</td>\n",
       "      <td>1</td>\n",
       "      <td>72.360</td>\n",
       "      <td>69.580</td>\n",
       "      <td>72.360</td>\n",
       "      <td>78.203</td>\n",
       "      <td>69.972</td>\n",
       "      <td>72.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>convnext</td>\n",
       "      <td>originwithaugment</td>\n",
       "      <td>4</td>\n",
       "      <td>72.360</td>\n",
       "      <td>70.887</td>\n",
       "      <td>72.360</td>\n",
       "      <td>79.737</td>\n",
       "      <td>71.342</td>\n",
       "      <td>77.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>convnext</td>\n",
       "      <td>mask</td>\n",
       "      <td>1</td>\n",
       "      <td>72.360</td>\n",
       "      <td>69.580</td>\n",
       "      <td>72.360</td>\n",
       "      <td>78.203</td>\n",
       "      <td>69.972</td>\n",
       "      <td>72.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>convnext</td>\n",
       "      <td>mask</td>\n",
       "      <td>2</td>\n",
       "      <td>72.050</td>\n",
       "      <td>67.721</td>\n",
       "      <td>72.050</td>\n",
       "      <td>76.608</td>\n",
       "      <td>67.478</td>\n",
       "      <td>74.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaint</td>\n",
       "      <td>2</td>\n",
       "      <td>72.050</td>\n",
       "      <td>68.361</td>\n",
       "      <td>72.050</td>\n",
       "      <td>77.045</td>\n",
       "      <td>68.787</td>\n",
       "      <td>74.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>convnext</td>\n",
       "      <td>inpaintwithaugment</td>\n",
       "      <td>2</td>\n",
       "      <td>72.050</td>\n",
       "      <td>68.361</td>\n",
       "      <td>72.050</td>\n",
       "      <td>77.045</td>\n",
       "      <td>68.787</td>\n",
       "      <td>74.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>convnext</td>\n",
       "      <td>maskwithaugment</td>\n",
       "      <td>2</td>\n",
       "      <td>72.050</td>\n",
       "      <td>67.721</td>\n",
       "      <td>72.050</td>\n",
       "      <td>76.608</td>\n",
       "      <td>67.478</td>\n",
       "      <td>74.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>convnext</td>\n",
       "      <td>mask</td>\n",
       "      <td>5</td>\n",
       "      <td>71.584</td>\n",
       "      <td>68.217</td>\n",
       "      <td>71.584</td>\n",
       "      <td>77.949</td>\n",
       "      <td>69.156</td>\n",
       "      <td>73.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>convnext</td>\n",
       "      <td>maskwithaugment</td>\n",
       "      <td>5</td>\n",
       "      <td>71.584</td>\n",
       "      <td>68.217</td>\n",
       "      <td>71.584</td>\n",
       "      <td>77.949</td>\n",
       "      <td>69.156</td>\n",
       "      <td>73.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>convnext</td>\n",
       "      <td>origin</td>\n",
       "      <td>1</td>\n",
       "      <td>71.429</td>\n",
       "      <td>66.962</td>\n",
       "      <td>71.429</td>\n",
       "      <td>77.053</td>\n",
       "      <td>67.969</td>\n",
       "      <td>73.694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model             Version  Fold  Accuracy  Precision  Sensitivity  \\\n",
       "4    convnext              origin     4    75.000     73.299       75.000   \n",
       "5    convnext  inpaintwithaugment     4    74.689     72.368       74.689   \n",
       "7    convnext             inpaint     4    74.689     72.368       74.689   \n",
       "8    convnext             inpaint     3    74.534     73.262       74.534   \n",
       "11   convnext  inpaintwithaugment     3    74.534     73.262       74.534   \n",
       "15   convnext              origin     3    74.224     71.385       74.224   \n",
       "32   convnext              origin     2    73.758     72.957       73.758   \n",
       "38   convnext             inpaint     5    73.602     70.174       73.602   \n",
       "39   convnext   originwithaugment     1    73.602     70.853       73.602   \n",
       "41   convnext  inpaintwithaugment     5    73.602     70.174       73.602   \n",
       "43   convnext   originwithaugment     2    73.447     71.023       73.447   \n",
       "47   convnext                mask     4    73.447     69.842       73.447   \n",
       "53   convnext   originwithaugment     3    73.447     71.191       73.447   \n",
       "54   convnext     maskwithaugment     4    73.447     69.842       73.447   \n",
       "58   convnext             inpaint     1    73.292     68.994       73.292   \n",
       "59   convnext   originwithaugment     5    73.292     71.170       73.292   \n",
       "61   convnext  inpaintwithaugment     1    73.292     68.994       73.292   \n",
       "85   convnext              origin     5    72.826     68.815       72.826   \n",
       "87   convnext                mask     3    72.826     70.136       72.826   \n",
       "88   convnext     maskwithaugment     3    72.826     70.136       72.826   \n",
       "104  convnext     maskwithaugment     1    72.360     69.580       72.360   \n",
       "107  convnext   originwithaugment     4    72.360     70.887       72.360   \n",
       "109  convnext                mask     1    72.360     69.580       72.360   \n",
       "119  convnext                mask     2    72.050     67.721       72.050   \n",
       "122  convnext             inpaint     2    72.050     68.361       72.050   \n",
       "123  convnext  inpaintwithaugment     2    72.050     68.361       72.050   \n",
       "124  convnext     maskwithaugment     2    72.050     67.721       72.050   \n",
       "144  convnext                mask     5    71.584     68.217       71.584   \n",
       "146  convnext     maskwithaugment     5    71.584     68.217       71.584   \n",
       "151  convnext              origin     1    71.429     66.962       71.429   \n",
       "\n",
       "     Specificity  F1-score     AUC  \n",
       "4         79.188    71.860  79.146  \n",
       "5         78.677    71.649  76.969  \n",
       "7         78.677    71.649  76.969  \n",
       "8         77.679    71.075  72.002  \n",
       "11        77.679    71.075  72.002  \n",
       "15        77.899    70.269  76.311  \n",
       "32        76.797    69.669  76.936  \n",
       "38        78.575    70.458  75.501  \n",
       "39        80.007    71.456  75.923  \n",
       "41        78.575    70.458  75.501  \n",
       "43        79.441    71.508  75.818  \n",
       "47        75.996    68.157  74.984  \n",
       "53        78.975    71.305  77.822  \n",
       "54        75.996    68.157  74.984  \n",
       "58        76.916    68.852  73.454  \n",
       "59        78.936    71.308  78.836  \n",
       "61        76.916    68.852  73.454  \n",
       "85        76.726    68.748  78.165  \n",
       "87        78.097    70.263  74.832  \n",
       "88        78.097    70.263  74.832  \n",
       "104       78.203    69.972  72.420  \n",
       "107       79.737    71.342  77.476  \n",
       "109       78.203    69.972  72.420  \n",
       "119       76.608    67.478  74.468  \n",
       "122       77.045    68.787  74.008  \n",
       "123       77.045    68.787  74.008  \n",
       "124       76.608    67.478  74.468  \n",
       "144       77.949    69.156  73.322  \n",
       "146       77.949    69.156  73.322  \n",
       "151       77.053    67.969  73.694  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "\n",
    "metrics = glob('/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics/all_*.csv')\n",
    "\n",
    "# 모든 csv 파일을 읽어서 DataFrame으로 변환\n",
    "all_metrics = pd.concat([pd.read_csv(metric) for metric in metrics], ignore_index=True)\n",
    "all_metrics.sort_values(by='Sensitivity', ascending=False, inplace=True, ignore_index=True)\n",
    "all_metrics.to_csv('/mnt/hdd/octc/PCOS_experiment/multi_v3/metrics/all_overall_metrics.csv')\n",
    "# convenxt만 뽑기\n",
    "convnext_metrics = all_metrics[all_metrics['Model'] == 'convnext']\n",
    "\n",
    "convnext_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ETC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.read_csv('/mnt/hdd/octc/experiment/binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "sys.path.append('../')\n",
    "import torchvision.transforms as transforms\n",
    "from lib.datasets.ds_tools import kfold_extract\n",
    "from lib.dataset import Custom_stratified_Dataset\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "\n",
    "from lib.datasets.sampler import class_weight_sampler, class_weight_getter\n",
    "from torch.utils.data import DataLoader\n",
    "folds, train_df, test_df  = kfold_extract('/mnt/hdd/octc/experiment/binary.csv',  5, True, random_state =627)\n",
    "for idx, fold in enumerate(folds):\n",
    "    print(f'Fold {idx+1}')\n",
    "    # fold안에 레이블 분포 확인\n",
    "    print(fold['train']['label'].value_counts())\n",
    "    print(fold['val']['label'].value_counts())\n",
    "    print('-------------------')\n",
    "    dataset = Custom_stratified_Dataset(\n",
    "        df = fold['train'],\n",
    "        root_dir = '/mnt/hdd/octc/BACKUP/Dataset',\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # sampler = class_weight_sampler(fold)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = 10,\n",
    "        sampler = ImbalancedDatasetSampler(dataset),\n",
    "        num_workers = 8\n",
    "    )\n",
    "    \n",
    "    for img, label in train_loader:\n",
    "        print(label.unique(return_counts=True))\n",
    "    print('-------------------')\n",
    "    # print(class_weight_getter(fold))\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    print('-------------------')\n",
    "    dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "sys.path.append('../')\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "device = 'cuda'\n",
    "from lib.datasets.ds_tools import kfold_extract\n",
    "from lib.dataset import Custom_stratified_Dataset\n",
    "from lib.datasets.sampler import class_weight_sampler, class_weight_getter\n",
    "from torch.utils.data import DataLoader\n",
    "folds, train_df, test_df  = kfold_extract('/mnt/hdd/octc/experiment/binary.csv',  5, True, random_state =627)\n",
    "for idx, fold in enumerate(folds):\n",
    "    print(f'Fold {idx+1}')\n",
    "    # fold안에 레이블 분포 확인\n",
    "    print(fold['train']['label'].value_counts())\n",
    "    print(fold['val']['label'].value_counts())\n",
    "    print('-------------------')\n",
    "    dataset = Custom_stratified_Dataset(\n",
    "        df = fold['train'],\n",
    "        root_dir = '/mnt/hdd/octc/BACKUP/Dataset',\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    n_pos = (fold['train']['label']==1).sum()\n",
    "    n_neg = (fold['train']['label']==0).sum()\n",
    "    pos_weight_value = n_neg / n_pos\n",
    "    pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to('cuda')\n",
    "\n",
    "    \n",
    "    print(pos_weight)\n",
    "    print('-------------------')\n",
    "    \n",
    "    \n",
    "    \n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "import pandas as pd\n",
    "def k_fold_split(csv_path, random_seed =42 ):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    groups = df['pid']  # Assuming 'pid' is the column for patient IDs\n",
    "    y = df['label']     # Assuming 'label' is the target variable\n",
    "\n",
    "    # folder = GroupKFold(n_splits=5)\n",
    "    folder = KFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "    folds = []\n",
    "\n",
    "    # for train_idx, val_idx in folder.split(df, y, groups):\n",
    "    for train_idx, val_idx in folder.split(df, y, groups):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "        fold = {'train': train_df, 'val': val_df}\n",
    "        folds.append(fold)\n",
    "    return folds\n",
    "\n",
    "folds = k_fold_split(csv_path = '/mnt/hdd/octc/experiment/dataset-train.csv')\n",
    "for fold in folds:\n",
    "    # train과 valid에서 겹치는 pid 개수 확인\n",
    "    print(len(set(fold['train']['pid'].unique()) & set(fold['val']['pid'].unique())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
